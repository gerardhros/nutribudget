---
title: "Procedure to do a meta analysis"
author: "Gerard H Ros"
date: "2023-08-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# load packages
require(data.table);require(metafor);require(readxl)

```

## The aim of a meta-analysis

First, a meta-analysis is always focussing on analysing the impact of treatment X on variable Y. So, it answers the question: how is variable Y changing due to the application of treatment X. The calculated change is also called “an effect  size”. This change can be expressed as relative or absolute change. Example:

*	The log response ratio: the logarithm of the mean of the treatment divided by mean of control
*	The hedges d: the absolute difference between the mean of the treatment and the control.

In addition, like all other regression models, you can estimate the impact of co-variables, being site properties or any other property that might affect the change of a treatment.

See these tutorials:

*	https://www.r-bloggers.com/2021/08/meta-analysis-in-r/
*	https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/
*	https://link.springer.com/article/10.1007/s11301-021-00247-4
*	https://www.tqmp.org/RegularArticles/vol11-1/p037/p037.pdf
*	https://www.metafor-project.org/doku.php/metafor
*	https://link.springer.com/book/10.1007/978-3-319-21416-0

In summary, a meta-analysis follows the next 8 steps:

1. Define the question you like to answer
2. Define the study selection criteria
3. Collect data from literature, and extend with open data if needed
4. Select and compute an effect size
5. Aggregate effect sizes per main factor
6. Do a meta-regression to quantify the impact of main factors and their interactions
7. Visualise impact of main factors and interactions
8. Apply the regression model on a new dataset (optional)

## Step 1. Define the question

In NutriBudget we’d like to know what the impact is of mitigation measures on a series of indicators. Indicators have been categorized as follows: pressure indicators, effect indicators and performance indicators. 

After finalizing the project we have a quantitative relationship between measures and impact via models of WP2, supported by meta-analytical regression models from WP1. The regression models of WP1 will support the calibration and validation of the roadmaps tested and evaluated in WP2. For items that can not be modelled with WP2, these regression models can also be applied. For now, all the 9 KPI indicators can be (and likely will be) simulated with NutriEurope and NutriFarm. 
For WP1 we focus first on the impact of measures on main KPIs. To start with, I would recommend to focus on 

1. Crop yield (and associated nutrient uptake, field level)
2. Nutrient surplus (both on field and farm level) for all nutrients
3. Nutrient Use Efficiency (both on field and farm level)
4. Soil properties pH and SOC (field level)

Clearly define for each KPI the control and treated situation.

## Step 2. Define selection criteria

Based on the objectives of Nutribudget we can apply filters on the searched papers. All papers are shortly screened and included or excluded using a series of criteria. These (might) include:

*	Statistical data: mean and standard deviations are perquisite, number of replicates should preferably known.
*	Spatial coverage: focus on Europe or comparable climate zones
*	Type and duration: we include only field experiments for a given duration
*	Experimental data: basic site properties should be known.

## Step 3. Data collection

To assess the impact of a measure, we need first to know:

*	The mean, standard deviation and number of replicates for a treatment (a measure being applied) and a control (no measure applied)
*	Additional site properties affecting the impact of a measure. This includes: farming system, soil properties, climatic conditions, land use, housing system (see the note on KPI and data).
*	Collect latitude and longitude of the experiment (to estimate missing data if needed).

Note that missing data can be retrieved from other sources. For example, in most studies the crop yield is known. Even when no nutrient uptake is measures, you can multiply the yield with a given crop composition and as such determine the nutrient uptake. The nutrient fertilization is usually known, so you can estimate the nutrient surplus in these cases. See the script `covariate extraction` in scripts

Note: the data structure usually needed is a row-based csv or excel file where each row represents an unique case with a mean, SD and n for both control and treatment as well all kind of site properties being a column in the table. The relevant items have been included in the measurement catalogue (where you only collect site properties from which you know they have an impact on the studies impact vs. measure combination).

Always check your data before starting a meta-analysis. These include:

* Check whether the variables are normally distributed, and if not, apply a transformation algorithm like a log function. 
* Check also the number of categories in a categorial variable. If the distribution of samples over the categories is really skewed, update the categories. 
* If you expect non-linear patterns between response variable and the independent variables, then add them as new variables.

When data in the publications are not given as SD but as t-test values, r-values, ANOVA, then it is possible to estimate the uncertainty from these values.
See: https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/es-calc.html

When data on SD are finally missing, then you can still use the data. In that case you can estimate it from the mean coefficient of variation retreived from other studies. See illustrated below for an example dataset from Luncheng Yu (available in `data`).

```{r step 3 example}

  # require packages (is already done, but not visible in html)
  # require(data.table); require(readxl)

  # read data from Excel and convert into a data.table (note that R markdown sets the working directory in the location of the file)
  d1 <- readxl::read_xlsx('../data/luncheng_2023_nue.xlsx',sheet = "Tables")
  d1 <- as.data.table(d1)

  # when data on variance (SD) is missing for the NUE of the control and treated plot, estimate this from the CV of the other studies
  d1[, nuet_cv := mean(nuet_sd/nuet_mean,na.rm=T) * 1.25]
  d1[, nuec_cv := mean(nuec_sd/nuec_mean,na.rm=T) * 1.25]
  d1[is.na(nuet_sd), nuet_sd := nuet_mean * nuet_cv]
  d1[is.na(nuec_sd), nuec_sd := nuec_mean * nuec_cv]

  # update the missing values for n_dose (replace by median N dose when missing)
  d1[is.na(n_dose), n_dose := median(d1$n_dose,na.rm=TRUE)]

  # NUE might follow a non-linear pattern, so add a variable N dose ^2
  d1[,n_dose2 := n_dose^2]
  
  # one might do a check to ensure normality of the variable, look to a histogram per variable, and if skewed, apply a log-transformation  
  d1[, clay := log(clay)]
  
  # # scale the variables to unit variance
  d1[,clay_scaled := scale(clay)]
  d1[,soc_scaled := scale(soc)]
  d1[,ph_scaled := scale(ph)]
  d1[,mat_scaled := scale(mat)]
  d1[,map_scaled := scale(map)]
  d1[,n_dose_scaled := scale(n_dose)]
  
```


## Step 4. Select and compute an effect size
The most common effect sizes are the log transformed response ratio, the mean difference and the standardized mean difference. The first one is the most simple one (since a relative change has no units) but also difficult to interpret spatially given the current and desired status. Preferably we use the standardized mean difference. 
This can easily be calculated via meta-analysis toolboxes like MetaWin or Metafor. I usually use the latter. It requires as input the mean, standard deviation and number of replicates for both the control and the treatment.
Note that this will be done for each KPI separately (so, the KPI is the response variable where you’re interested in). The same is true for all other sections below. 

See: https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/effects.html


```{r step 4 example}

  # require package Metafor (is already done, but not visible in html)
  # require(metafor)

  # calculate the effect size log-transformed response ratio (ROM)
  es21 <- escalc(measure = "ROM", data = d1, 
                   m1i = nuet_mean, sd1i = nuet_sd, n1i = replication,
                   m2i = nuec_mean, sd2i = nuec_sd, n2i = replication )

  # calculate the effect size for raw mean difference
  es21 <- escalc(measure = "MD", data = d1, 
                   m1i = nuet_mean, sd1i = nuet_sd, n1i = replication,
                   m2i = nuec_mean, sd2i = nuec_sd, n2i = replication )
  
  # calculate the effect size for standardized mean difference
  es21 <- escalc(measure = "SMD", data = d1, 
                   m1i = nuet_mean, sd1i = nuet_sd, n1i = replication,
                   m2i = nuec_mean, sd2i = nuec_sd, n2i = replication )

```

## Step 5. Aggregate effect sizes per factor

When for a series of papers data is collected regarding the change in an indicator (the effect size) due to the application of a treatment, then one can estimate the mean effect across land uses, soil types, climate zones, soil health categories, and so on. This is often referred to as a “main factor analysis” where you do not look at interactions among variables, but just to explore how the impact of a measure depends on the aforementioned factors. For each of them, you will get a mean impact (by group) with a given uncertainty. This tells you how these factors affect the impact of a treatment.
Mathematically it can be summarized as: Y = A * X + error, where you like to quantify the value A for specific groups / factors that affect the change in the indicator (being the response variable Y).

See: https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/pooling-es.html
See: https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/subgroup.html

As a start i usually do a main-factor analysis to get a first overview on the impact of all independent variables on the response variable.
The output is saved into one object, that can be used for plotting later but at least for the selection of relevant variables for further analysis.

For more detailed illustration, see `analysis nue luncheng.R`, available in `scripts`

```{r step 5 example, eval = FALSE}

  # convert to data.table
  d02 <- as.data.table(es21)

  # what are the treatments to be assessed
  d02.treat <- data.table(treatment =  c('ALL',unique(d02$management)))
  
  # add description of the labels, simplified in this case
  d02.treat <- d02.treat[1:4]
  d02.treat[treatment=='ALL',desc := 'All']
  d02.treat[treatment=='EE',desc := 'Enhanced Efficiency']
  d02.treat[treatment=='CF',desc := 'Combined fertilizer']
  d02.treat[treatment=='RES',desc := 'Residue incorporation']

   # do a main factor analysis of the factors controlling the response variable
  # for more details on the function used, see ?rma.mv
  
  # add a list to store the coefficients
  out2 = out3 = list()

  # make a for loop to do a main analysis per treatment
  for(i in d02.treat$treatment){
    
    if(i=='ALL'){
      
      # run without selection to estimate overall mean, with a random error structured by studyID
      r_nue <- metafor::rma.mv(yi,vi, 
                               data=d02,
                               random= list(~ 1|studyid), 
                               method="REML",
                               sparse = TRUE)
      
    } else {
      
      # run for selected treatment with a random error structured by studyID
      r_nue <- metafor::rma.mv(yi,vi, 
                               data=d02[management==i,],
                               random= list(~ 1|studyid), 
                               method="REML",
                               sparse = TRUE)
      
    }
    
    # save relevant output in a list: the model coefficient (corrected to relative change in %), the standard error, the pvalue and the label
    out2[[i]] <- data.table(mean = as.numeric((exp(r_nue$b)-1)*100),
                            se = as.numeric((exp(r_nue$se)-1)*100),
                            pval = round(as.numeric(r_nue$pval),4),
                            label =  paste0(d02.treat[treatment==i,desc],' (n=',r_nue$k,')')
                            )
    # print to console to see the progress
    # print(paste0('treatment ',i,' has been simulated'))
  }

# convert all output from the list out2 to a data.table
out2 <- rbindlist(out2)

# see the object
print(out2)
  

```

## Step 6. Do a meta-regression to quantify the impact of main factors and their interactions

After the main factor analysis, we can make a regression model that accounts for interactions among the site conditions controlling the impact of a measure. 
Mathematically it can be summarized as: Y = A * X1 + B * X2 + C*X3 + D*X2*X3 + error, where you like to quantify the value A, B, C and D (the regression coefficients) for specific groups / factors (X1, X2 and X3) that affect the change in the indicator (being the response variable Y).
This can easily be calculated via meta-analysis toolboxes like MetaWin or Metafor.
See: https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/metareg.html
See: https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/multilevel-ma.html

To compare models with each other i first made a short summary function to retreive statistical data from the models developed, and to evaluate the model performance. There are more performance metrices available, and you can adapt this helper function, but for the first approach, this is sufficient. It firstly derives the Aiken criteria (AIC), then the log likelyhood (ll), then the improvement in ll compared to an empty baseline model (so, a model without explanatory variables), and the change in explained variance (R2), and it gives the p-value. This implies that a model performance is compared with a baseline model.

```{r meta regression helper}

# make a function to extract relevant model statistics
  estats <- function(model_new,model_base){
         out <- data.table(AIC = model_new$fit.stats[4,2],
                           ll = model_new$fit.stats[1,2],
                           ll_impr = round(100 * (1-model_new$fit.stats[1,2]/model_base$fit.stats[1,2]),2),
                           r2_impr = round(100*max(0,(sum(model_base$sigma2)-sum(model_new$sigma2))/sum(model_base$sigma2)),2),
                           pval = round(anova(model_new,model_base)$pval,3))
     return(out)
  }

```

Then we can start with building a meta-regression model. On average, there are two approaches. 

* first, one can add all variables for which one expects an impact on the response variable. After analysing the contribution of these individual variables (via `summary()`) one can deduct which of the variables has a significant influence on the effect size and which one not. By adding interaction terms one can assess whether the model performance can be improved, and how the different site properties and management practices affect the NUE (in this case). Insignificant main factors are not removed, but only significant interactions are included. An insignificant factors can then be explained that there is no obvious relationship between the response variable (the effect size) and the explanatory X variable. 
* second, one can start with all significant variables, and then remove all insignificant ones after the first check of their contribution. Check subsequently possible interactions among the significant ones, and leave significant ones in the model. As soon as a predraft model is ready (no improvements possible in R2 or ll or AIC) and checking the additional value of the earlier removed explanatory variables to this predraft model does not help improving the model perfomance, then you can consider this one as the final model to be used.

Note that in linear regression models in R, the `*` sign means main factors plus interactions. Thus when the response variable can be explained by factors A and B, then a full interaciton model can be written as `Y ~ A*B` being equal to `Y = A + B + A x B`. When you are ONLY interested in the interaction term, then you can make use of the `:` sign, in this case `Y = A:B` being equal to `Y = A x B`. 

When you analyse the summary stats of this model (r_nue_5 in this case) then its clear that fertilizer type and fertilizer do have a strong impact on the NUE wheras the site factors had only limited impact.  

```{r meta regression}

  # run without a main factor selection to estimate overall mean
  r_nue_0 <- rma.mv(yi,vi, data = d02,random= list(~ 1|studyid), method="REML",sparse = TRUE)

  # show there the final model without interactions but with only main factor impacts. Note the `-1` in the end, meaning that you do not include an   
  # intercept. That is allowed since there are categorial variables included.
  r_nue_5 <- rma.mv(yi,vi, 
                  mods = ~fertilizer_type + fertilizer_strategy +  crop_residue + tillage + cover_crop_and_crop_rotation + g_crop_type + 
                          n_dose_scaled + soc_scaled + clay_scaled + ph_scaled + map_scaled + mat_scaled - 1, 
                  data = d02,
                  random = list(~ 1|studyid), method="REML",sparse = TRUE)
  
  # show stats and improvements
  out = estats(model_new = r_nue_5,model_base = r_nue_0)
  print(paste0('model improved the log likelyhood with ',round(out$ll_impr,1),'%'))
  summary(r_nue_5)
  
  # see here another example, where some categorial variables were one-hot encoded
  d02[,r4pl := fifelse(fertilizer_strategy=='placement','yes','no')]
  d02[,r4ti := fifelse(fertilizer_strategy=='timing','yes','no')]
  d02[,r4do := fifelse(fertilizer_strategy=='rate','yes','no')]
  d02[,ctm := fifelse(g_crop_type=='maize','yes','no')]
  d02[,ctw := fifelse(g_crop_type=='wheat','yes','no')]
  d02[,ctr := fifelse(g_crop_type=='rice','yes','no')]
  d02[,cto := fifelse(g_crop_type=='other','yes','no')]
  d02[,ndose2 := scale(n_dose^2)]
  
  # this is another model with only studyid to the error structure but with some interactions included (just as a random example)
  # note variables that do not have variation (eg. cto) are by definition not useful, they can never explain variation in Y since there is no variation in X.
  r_nue_5 <- rma.mv(yi,vi,
                  mods = ~fertilizer_type + r4ti + r4do + crop_residue + tillage +
                    cover_crop_and_crop_rotation + n_dose_scaled + clay_scaled * ph_scaled + map_scaled + mat_scaled +
                    soc_scaled : n_dose_scaled  +ctm:r4pl+ ctm + ctw + ctr + ctm:mat_scaled + ndose2 -1,
                  data = d02,
                  random = list(~ 1|studyid), method="REML",sparse = TRUE)
   out = estats(model_new = r_nue_5,model_base = r_nue_0)
   print(paste0('model improved the log likelyhood with ',round(out$ll_impr,1),'%'))
   
   # note that you can also add more structure to the error component: here by adding crop type to the error structure
   r_nue_5n <- rma.mv(yi,vi,
                  mods = ~fertilizer_type + r4pl + r4ti + r4do + crop_residue + tillage +
                    cover_crop_and_crop_rotation + n_dose_scaled + clay_scaled + ph_scaled + map_scaled + mat_scaled +
                    soc_scaled : n_dose_scaled  + ctm:r4pl+ ctm + ctw + ctr + ctm:mat_scaled + ndose2 -1,
                  data = d02,
                  random = list(~ 1|studyid/g_crop_type), method="REML",sparse = TRUE)

   out = estats(model_new = r_nue_5,model_base = r_nue_0)
   print(paste0('model improved the log likelyhood with ',round(out$ll_impr,1),'%'))
   
```


## step 7. Visualise impact of main factors and interactions

There are all kind of visualisations that can be made to show the performance of the model, the main factors controlling the impact of measures, and so on. Get inspired by a few examples.
See: https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/forest.html

## step 8. Apply the model

To apply the model on spatial datasets one has first to prepare spatial raster files for all relevant properties that are needed in the model developed in step 6. Below some examples from datasources retreived from literature, where the data are preprocessed and stored as tif file in this repo. The code is shown how these files are prepared, however the source data is not given due to memory reasons in git. Note that i show here some maps for global application, but for all variables similar maps can be prepared on EU level using datasets from INTEGRATOR, LUCAS and other EU data sources.

```{r prepare global raster maps}
  
  # load climatic data
  if(FALSE){

    # downloaded via CRU, https://catalogue.ceda.ac.uk/uuid/89e1e34ec3554dc98594a5732622bce9
    rfiles <- list.files('D:/DATA/02 climate',pattern = 'tif|nc',full.names = T)
    rfiles <- rfiles[grepl('pre|tmp',rfiles)]
    clim <- terra::sds(rfiles)
    clim <- terra::rast(clim)

    clim.dt <- as.data.frame(clim,xy=TRUE)
    clim.dt <- as.data.table(clim.dt)

    # rearrange data
    d2.clim <- melt(clim.dt,id.vars = c('x','y'), variable.name = 'variable')
    d2.clim <- d2.clim[!grepl('_stn_',variable)]
    d2.clim[, cvar :=  stringr::str_extract_all(variable,"(?<=[0-9]{4}\\.[0-9]{4}\\.).+(?=\\.dat_)",simplify = T)]
    d2.clim[, years :=  stringr::str_extract_all(variable,"[0-9]{4}\\.[0-9]{4}",simplify = T)]
    d2.clim[, month :=  stringr::str_extract_all(variable,"(?<=[a-z]{3}_)\\d+",simplify = T)]

    # estimate mean global climate properties over 1991-2019
    # temperature in degrees (mean = tmp) and precipitation (pre) in mm/month
    d3.clim <- dcast(d2.clim,x+y+years+month~cvar,value.var = 'value')
    d3.clim[,month := as.numeric(month)]
    d3.clim[, year := floor((month - 1)/ 12) + as.numeric(substr(years,1,4))]

    # derive the mean and SD per gridcel over period 1991-2019
    d4.clim <- d3.clim[,list(mat = mean(tmp),pre = sum(pre)),by=c('x','y','year')]
    d4.clim <- d4.clim[,list(mat = mean(mat),pre = mean(pre)),by=c('x','y')]

    # set back to raster
    r.clim <- terra::rast(d4.clim,type='xyz')
    terra::crs(r.clim) <- 'epsg:4326'

    # write raster with MAT and MAP to disk
    terra::writeRaster(r.clim,'data/climate.tif', overwrite = TRUE)

  } else {

    # read the raster with climatic data
    r.clim <- terra::rast('data/climate.tif')

  }


  # load SPAM raster
  # spam <- as.data.table(foreign::read.dbf('D:/DATA/04 crop/spam2010/cell5m_allockey_xy.dbf'))

  # load crop area harvested as raster
  if(FALSE){

    # get the file names of the tiffs
    rfiles <- list.files('D:/DATA/04 crop/spam2010', pattern = '_H.tif$',full.names = TRUE)

    # read in all files and convert to spatrasters
    r.crop <- sds(rfiles)
    r.crop <- rast(r.crop)

    # aggregate to 0.5 x 0.5 degree
    r.crop <- terra::aggregate(r.crop,fact = 0.5/0.083333,fun = "sum", na.rm=T)

    # adjust names
    names(r.crop) <- stringr::str_extract(names(r.crop),'[A-Z]{4}')

    # crop names to combine in other
    scrop <- c('RICE','MAIZ')
    wheat <- c('WHEA','BARL','SMIL','PMIL')
    other <- names(r.crop)[!names(r.crop) %in% c(scrop,wheat)]

    # sum all other crops
    r.crop.other = app(r.crop[[other]],fun = sum,na.rm=T)
    names(r.crop.other) <- 'other'
    r.crop.wheat = app(r.crop[[wheat]],fun = sum,na.rm=T)
    names(r.crop.wheat) <- 'wheat'

    # combine again
    r.crop <- c(r.crop[[scrop]],r.crop.other,r.crop.wheat)

    # reproject to r.clim
    r.crop <- resample(r.crop,r.clim,method='bilinear')

    # write raster
    terra::writeRaster(r.crop,'data/ma_crops.tif', overwrite = TRUE)


  } else {

    # read the raster with cropping data
    r.crop <- rast('data/ma_crops.tif')

  }

  # load the relevant soil properties
  if(FALSE){

    # load in ph and clay as raster, WGS84, 4326, 0.5 x 0.5 degrees
    ph <- terra::rast('D:/DATA/01 soil/isric_phw_mean_0_5.tif')
    clay <- terra::rast('D:/DATA/01 soil/isric_clay_mean_0_5.tif')
    soc <-  terra::rast('D:/DATA/01 soil/isric_soc_mean_0_5.tif')
    soc <- terra::resample(soc,clay)

    # combine both in one raster
    r.soil <- c(ph,clay,soc)

    # reproject to r.clim
    r.soil <- terra::resample(r.soil,r.clim,method='bilinear')

    # write raster
    terra::writeRaster(r.soil,'data/soil.tif', overwrite = TRUE)

  } else {

    # read the raster with soil data
    r.soil <- terra::rast('data/soil.tif')
  }


  # prepare raster with tillage practices
  # source: https://dataservices.gfz-potsdam.de/pik/showshort.php?id=escidoc:4085895
  # https://datapub.gfz-potsdam.de/download/10.5880.PIK.2019.011/
  if(FALSE){

    # read in the rasters from nc4 file
    tillage <- terra::rast('D:/DATA/05 tillage/tillage_revised.nc4')
    terra::crs(tillage) <- 'epsg:4326'

    # tillage categories
    # 1 = Conventional annual tillage
    # 2 = Traditional annual tillage
    # 3 = Reduced tillage
    # 4 = Conservation Agriculture
    # 5 = Rotational tillage
    # 6 = Traditional rotational tillage
    # 7 = Scenario Conservation Agriculture area

    # change names
    names(tillage) <- c("WHEA","RICE","MAIZ","BARL","REST","OOIL","TOBA","TEAS","COCO","RCOF","ACOF","OFIB","COTT","SUGB","SUGC","OILP","VEGE",
                        "TEMF","TROF","PLNT","BANA","CNUT","GROU","OTRS","CASS","YAMS","SWPO","POTA","SESA","RAPE","SUNF","SOYB","OPUL","LENT",
                        "PIGE","COWP","CHIC","BEAN","OCER","SORG","SMIL","PMIL","scenario_ca_area")

    # crop names to combine in other
    scrop <- c('RICE','MAIZ')
    wheat <- c('WHEA','BARL','SMIL','PMIL')
    other <- names(tillage)[!names(tillage) %in% c(scrop,wheat)]

    # select the most frequent tillage pracice for all other crops
    r.till.other = app(tillage[[other]],fun = modal,na.rm=T)
    names(r.till.other) <- 'other'
    r.till.wheat = app(tillage[[wheat]],fun = modal,na.rm=T)
    names(r.till.wheat) <- 'wheat'

    # combine again
    r.till <- c(tillage[[scrop]],r.till.other,r.till.wheat)

    # reproject to r.clim
    r.till <- resample(r.till,r.clim,method='near')

    # write raster with tillage to disk
    terra::writeRaster(r.till,'data/tillage.tif', overwrite = TRUE)

  } else {

    # read the earlier prepared file with tillage practices
    r.till <- terra::rast('data/tillage.tif')

  }

  # prepare manure N dose on cropland (take latest year present)
  # source: https://doi.pangaea.de/10.1594/PANGAEA.871980
  # units: kg N / km2 / yr
  if(FALSE){

    # read the file with manure application data (kg N / km2 grid cell)
    nman <- data.table::fread('D:/DATA/06 inputs/DN_yy2010.txt')
    nman <- terra::rast(nrows=2124,ncols=4320,xmin=-180,ymin=-88.5,crs = "+proj=longlat +datum=WGS84",
                        vals=as.matrix(nman))

    # retreive values for new raster object
    r.nman <- terra::resample(nman,r.clim[[1]],method='bilinear')

    names(r.nman) <- 'nofert'

    # write raster with manure N dose to cropland to disk
    terra::writeRaster(r.nman,'data/nofert.tif', overwrite = TRUE)

  } else {

    # read the earlier prepared file with manure N dose
    r.nman <- terra::rast('data/nofert.tif')
  }

  # fertilizer N dose, NH4 and NO3, 0.5 x 0.5 resolution, 1961-2010
  # https://doi.pangaea.de/10.1594/PANGAEA.861203
  # units: kg N /ha cropland (50 years since 1960, take the last one)
  if(FALSE){

    # load total N fertilizer dose (kg N / ha of gri area) and take the latest year
    nfert_no3 = terra::rast('D:/DATA/06 inputs/NO3_input_ver1.nc4')
    nfert_no3 = nfert_no3[[589:600]]
    nfert_nh4 = terra::rast('D:/DATA/06 inputs/NH4_input_ver1.nc4')
    nfert_nh4 = nfert_nh4[[589:600]]

    # sum the monthly values
    nfert_nh4 = app(nfert_nh4,fun = sum,na.rm=T)
    nfert_no3 = app(nfert_no3,fun = sum,na.rm=T)
    names(nfert_nh4) <- 'nfert_nh4'
    names(nfert_no3) <- 'nfert_no3'

    # add both together
    r.nfert <- c(nfert_nh4,nfert_no3)

    # retreive values for new raster object
    r.nfert <- terra::resample(r.nfert,r.clim,method='bilinear')

    # write raster with inorganic N fertilization to disk
    terra::writeRaster(r.nfert,'data/nifert.tif', overwrite = TRUE)


  } else {

    # read the earlier prepared file with fertilizer N dose
    r.nfert <- terra::rast('data/nifert.tif')
  }

# read in crop rotation intensity derived from MODIS (from Liu et al 2021)
# works for arable systems, but not so well for grassland (manual check GR)
# https://doi.org/10.6084/m9.figshare.14099402
  if(FALSE){

    # read in the raster from year 2010
    r1 <- terra::rast('D:/DATA/04 crop/rotation/GCI_QC_2010.tif')

    # cropping intensity of 0 is NA
    r1[r1==0] <- NA

    # lower resolution to 0.5x0.5 degree
    r1.cropint <- terra::aggregate(r1,fact = 0.5/0.002209708,fun='mean',na.rm=TRUE)

    # resample to similar format as other files
    r1.cropint <-  terra::resample(r1.cropint,r.clim,method='near')

    # apply a round function
    r1.cropint <- app(r1.cropint,fun = round)
    names(r1.cropint) <- 'cropintensity'

    # write raster with inorganic N fertilization to disk
    terra::writeRaster(r1.cropint,'data/cropintensity.tif', overwrite = TRUE)

  } else {

    # read the earlier prepared file with cropping intensity
    r1.cropint <- terra::rast('data/cropintensity.tif')
  }

```

When the datasets are available (and they are, see the directory data) then one can read the rasters and convert them into a single data.table.
That data.table can then be used as input for the meta-regression model to be applied.

```{r application meta-regression}

 # read all rasters

    # what rasters files that are stored in directory data
    rfiles <- list.files('data', pattern = 'tif$',full.names = TRUE)
    rfiles <- rfiles[!grepl('cropland',rfiles)]

    # read in raster files
    r.ma <- terra::sds(rfiles)

    # convert to raster
    r.ma <- terra::rast(r.ma)

# convert rasters to data.table

    # set first to xy data.frame (NA=FALSE otherwise gridcels are removed)
    r.df <- as.data.frame(r.ma,xy = TRUE, na.rm = FALSE)

    # convert to data.table
    r.dt <- as.data.table(r.df)

    # update the column names to easier ones to refer to
    setnames(r.dt,old = c('climate_mat', 'climate_pre','soil_isric_phw_mean_0_5','soil_isric_clay_mean_0_5','soil_isric_soc_mean_0_5',
                          'nifert_nfert_nh4','nifert_nfert_no3','nofert_nofert','cropintensity_cropintensity'),
             new = c('mat','pre','phw','clay','soc','nh4','no3','nam','cropintensity'),skip_absent = T)

    # select only land area, so skip all data points that are not relevant for this study
    r.dt <- r.dt[!(is.na(mat)|is.na(pre))]
    r.dt <- r.dt[!(is.na(tillage_RICE) & is.na(tillage_MAIZ) & is.na(tillage_other) & is.na(tillage_wheat))]
    r.dt <- r.dt[!(is.na(ma_crops_RICE) & is.na(ma_crops_MAIZ) & is.na(ma_crops_other) & is.na(ma_crops_wheat))]

    # replace area with 0 when missing
    cols <- colnames(r.dt)[grepl('^ma_|nh4|no3|nam',colnames(r.dt))]
    r.dt[,c(cols) := lapply(.SD,function(x) fifelse(is.na(x),0,x)),.SDcols = cols]
    cols <- colnames(r.dt)[grepl('^tillage',colnames(r.dt))]
    r.dt[,c(cols) := lapply(.SD,function(x) fifelse(is.na(x),1,x)),.SDcols = cols]
    r.dt[is.na(cropintensity), cropintensity := 1]

    # melt the data.table
    r.dt.melt <- melt(r.dt,
                      id.vars = c('x','y','mat', 'pre','phw','clay','nh4','no3','nam','soc','cropintensity'),
                      measure=patterns(area="^ma_crops", tillage ="^tillage_"),
                      variable.factor = FALSE,
                      variable.name = 'croptype')

    # set the crop names (be aware, its the order in ma_crops)
    r.dt.melt[,cropname := c('rice','maiz','other','wheat')[as.numeric(croptype)]]

    # set names to tillage practices
    r.dt.melt[, till_name := 'conventional']
    r.dt.melt[tillage %in% c(3,4,7), till_name := 'no-till']

# derive a meta-analytical regression model
    
    # read data (if you run this code in Rmd then its fine, if you run it via console, add '../' before data to ensure correct working directory)
    d1 <- readxl::read_xlsx('data/luncheng_2023_nue.xlsx',sheet = 1)
    d1 <- as.data.table(d1)

    # add CV for NUE and estimate the SD for missing ones
    d1[, cv_nuet := nuet_sd / nuet_mean]
    d1[is.na(nuet_sd),nuet_sd := mean(d1$cv_nuet,na.rm=TRUE) * 1.25]

    # add CV for N-uptake and estimate the SD for missing ones
    d1[, cv_nuec := nuec_sd / nuec_mean]
    d1[is.na(nuec_sd),nuec_sd := mean(d1$cv_nuec,na.rm=TRUE) * 1.25]

    # clean up column names
    setnames(d1,gsub('\\/','_',gsub(' |\\(|\\)','',colnames(d1))))
    setnames(d1,tolower(colnames(d1)))

    # calculate effect size (NUE)
    es21 <- escalc(measure = "ROM", data = d1,
                   m1i = nuet_mean, sd1i = nuet_sd, n1i = replication,
                   m2i = nuec_mean, sd2i = nuec_sd, n2i = replication )

    # convert to data.tables
    d02 <- as.data.table(es21)

    # what are the treatments to be assessed
    d02.treat <- data.table(treatment =  c('ALL',unique(d02$management)))

    # what are labels
    d02.treat[treatment=='ALL',desc := 'All']
    d02.treat[treatment=='EE',desc := 'Enhanced Efficiency']
    d02.treat[treatment=='CF',desc := 'Combined fertilizer']
    d02.treat[treatment=='RES',desc := 'Residue retention']
    d02.treat[treatment=='RFP',desc := 'Fertilizer placement']
    d02.treat[treatment=='RFR',desc := 'Fertilizer rate']
    d02.treat[treatment=='ROT',desc := 'Crop rotation']
    d02.treat[treatment=='RFT',desc := 'Fertilizer timing']
    d02.treat[treatment=='OF',desc := 'Organic fertilizer']
    d02.treat[treatment=='RT',desc := 'Reduced tillage']
    d02.treat[treatment=='NT',desc := 'No tillage']
    d02.treat[treatment=='CC',desc := 'Crop cover']
    d02.treat[treatment=='BC',desc := 'Biochar']

    # update the missing values for n_dose and p2o5_dose (as example)
    d02[is.na(n_dose), n_dose := median(d02$n_dose,na.rm=TRUE)]

    # scale the variables to unit variance
    d02[,clay_scaled := scale(clay)]
    d02[,soc_scaled := scale(soc)]
    d02[,ph_scaled := scale(ph)]
    d02[,mat_scaled := scale(mat)]
    d02[,map_scaled := scale(map)]
    d02[,n_dose_scaled := scale(n_dose)]

    # Combining different factors
    d02[g_crop_type=='vegetable', g_crop_type := 'other']
    d02[tillage == 'reduced', tillage := 'no-till']
    d02[fertilizer_type=='organic', fertilizer_type := 'organic_and_combined']
    d02[fertilizer_type=='combined', fertilizer_type := 'organic_and_combined']

    # make metafor model
    m1 <- rma.mv(yi,vi,
                mods = ~fertilizer_type + fertilizer_strategy + crop_residue + tillage + cover_crop_and_crop_rotation + g_crop_type + 
                        n_dose_scaled + clay_scaled + ph_scaled + map_scaled + mat_scaled + soc_scaled : n_dose_scaled - 1,
                data = d02,
                random = list(~ 1|studyid), method="REML",sparse = TRUE)

# how to use the same model in predicton mode
    
    # see model structure that need to be filled in to predict NUE as function of the system properties
    p1 <- predict(m1,addx=T)

    # this is the order of input variables needed for model predictions (=newmods in predict function)
    m1.cols <- colnames(p1$X)

    # make prediction dataset for situation that soil is fertilized by both organic and inorganic fertilizers, conventional fertilizer strategy
    dt.new <- copy(r.dt.melt)

    # add the columns required for the ma model, baseline scenario
    # baseline is here defined as "strategy conventional", combined organic and mineral fertilizers, no biochar, no crop residue, no cover crops
    dt.new[, fertilizer_typeenhanced := 0]
    dt.new[, fertilizer_typemineral := 0]
    dt.new[, fertilizer_typeorganic_and_combined := 1]
    dt.new[, fertilizer_strategyplacement := 0]
    dt.new[, fertilizer_strategyrate := 0]
    dt.new[, fertilizer_strategytiming := 0]
    dt.new[, biocharyes := 0]
    dt.new[, crop_residueyes := 0]
    dt.new[, cover_crop_and_crop_rotationyes := fifelse(cropintensity>1,1,0)]
    dt.new[, `tillageno-till` := fifelse(till_name =='no-till',1,0)]
    dt.new[, g_crop_typeother := fifelse(cropname=='other',1,0)]
    dt.new[, g_crop_typerice := fifelse(cropname=='rice',1,0)]
    dt.new[, g_crop_typewheat := fifelse(cropname=='wheat',1,0)]
    dt.new[, ph_scaled := (phw * 0.1 - mean(d02$ph)) / sd(d02$ph)]
    dt.new[, clay_scaled := (clay * 0.1 - mean(d02$clay)) / sd(d02$clay)]
    dt.new[, soc_scaled := (soc * 0.1 - mean(d02$soc)) / sd(d02$soc)]
    dt.new[, n_dose_scaled := scale(nh4+no3+nam)]
    dt.new[, map_scaled := (pre - mean(d02$map)) / sd(d02$map)]
    dt.new[, mat_scaled := (mat  - mean(d02$mat)) / sd(d02$mat)]
    dt.new[, `n_dose_scaled:soc_scaled` := n_dose_scaled*soc_scaled]

    # convert to matrix, needed for rma models
    dt.newmod <- as.matrix(dt.new[,mget(c(m1.cols))])

    # predict the NUE via ROM model
    dt.pred <- as.data.table(predict(m1,newmods = dt.newmod,addx=F))

    # add predictions to the data.table
    cols <- c('pROMmean','pROMse','pROMcil','pROMciu','pROMpil','pROMpiu')
    dt.new[,c(cols) := dt.pred]

    # change ROM into the relative change due to a measure
    dt.new[,pROMmeanAbs := (exp(pROMmean) - 1) * 100]
    
 # scenario 1. the best combination of measures witout change in crop rotation

    # make local copy
    dt.s1 <- copy(dt.new)

    # update actions taken for scenario 1
    dt.s1[, fertilizer_typeenhanced := 1]
    dt.s1[, fertilizer_typeorganic_and_combined := 1]
    dt.s1[, fertilizer_strategyplacement := 1]
    dt.s1[, crop_residueyes := 1]
    dt.s1[, cover_crop_and_crop_rotationyes := 1]
    dt.s1[, ph_scaled := (phw * 0.1 - mean(d02$ph)) / sd(d02$ph)]
    dt.s1[, clay_scaled := (clay * 0.1 - mean(d02$clay)) / sd(d02$clay)]
    dt.s1[, soc_scaled := (soc * 0.1 - mean(d02$soc)) / sd(d02$soc)]
    dt.s1[, n_dose_scaled := scale(nh4+no3+nam) - 0.1]
    dt.s1[, map_scaled := (pre - mean(d02$map)) / sd(d02$map)]
    dt.s1[, mat_scaled := (mat  - mean(d02$mat)) / sd(d02$mat)]
    dt.s1[, `n_dose_scaled:soc_scaled` := (n_dose_scaled - 0.1 )*soc_scaled]

    # convert to matrix, needed for rma models
    dt.newmod <- as.matrix(dt.s1[,mget(c(m1.cols))])

    # predict the NUE via ROM model
    dt.pred.s1 <- as.data.table(predict(m1,newmods = dt.newmod,addx=F))
    dt.s1[,c(cols) := dt.pred.s1]
    dt.s1[,pROMmeanAbs := (exp(pROMmean) - 1) * 100]
    
# compare baseline with scenario

    # select relevant columns of the baseline
    dt.fin <- dt.new[,.(x,y,base = pROMmeanAbs,cropname,area)]

    # select relevant columns of scenario 1 and merge
    dt.fin <- merge(dt.fin,dt.s1[,.(x,y,s1 = pROMmeanAbs,cropname)],by=c('x','y','cropname'))

    # estimate relative improvement via senario 1
    dt.fin[, improvement := s1 - base]

    # estimate area weighted mean relative improvement
    dt.fin <- dt.fin[,list(improvement = weighted.mean(improvement,w = area)),by = c('x','y')]
    
# make spatial raster of the estimated improvement

    # convert to spatial raster
    r.fin <- terra::rast(dt.fin,type='xyz')
    terra::crs(r.fin) <- 'epsg:4326'

    # write as output
    terra::writeRaster(r.fin,'products/scenario_1.tif', overwrite = TRUE)
```


## Literature

To see some inspiring blogs, examples and so on:

* https://cran.r-project.org/bin/windows/base/
* https://posit.co/download/rstudio-desktop/
* https://www.metafor-project.org/doku.php/metafor
* https://www.r-bloggers.com/2021/03/data-table-everything-you-need-to-know-to-get-you-started-in-r/
* https://s3.amazonaws.com/assets.datacamp.com/blog_assets/datatable_Cheat_Sheet_R.pdf
